# -*- coding: utf-8 -*-
"""Tijuana Houses Regression Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cOvfXOVh3-U-DYZ0UcB7dW0It98kbjp7
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

url = 'https://raw.githubusercontent.com/IvanVC21/Tijuana-house-prices/main/tijuana_prices.csv'
df = pd.read_csv(url)

df.head(10)

df.shape

df.isnull().sum().sort_values(ascending=False).head()

df['parkingSpots'].fillna(1 , inplace=True)

df.dropna(inplace=True)
df.isnull().sum()

print("% of neighborhoods listed as 'Tijuana, Baja California':", (df['neighborhood'].value_counts().max()/866)*100)

df = df.drop(df.index[df['neighborhood'] == 'Tijuana, Baja California Norte'])

df.shape

df_mn = df[df['currency'] == 'MN']
df_mn.head()

df_mn = df_mn.apply(lambda x: x / 20 if x.name == 'price' else x)
df_mn.head()

df = df.drop(df.index[df['currency'] == 'MN'])
df.shape

prices = [df, df_mn]
df = pd.concat(prices).sort_index()

df.head()

df = df.drop(columns = ['currency'])

df.dtypes

df.head()

df.insert(8, "latSY", 32.5414378)
df.insert(9, "lonSY", -117.0275893)
df.insert(10, "latOT", 32.5457130)
df.insert(11, "lonOT", -116.9379029)

df.dtypes

R = 6373.0
df['lat_rad'] = np.radians(df['lat'])
df['lon_rad'] = np.radians(df['lon'])
df['latSYrad'] = np.radians(df['latSY'])
df['lonSYrad'] = np.radians(df['lonSY'])
df['latOTrad'] = np.radians(df['latOT'])
df['lonOTrad'] = np.radians(df['lonOT'])

df['dlatSY'] = df['latSYrad'] - df['lat_rad']
df['dlonSY'] = df['lonSYrad'] - df['lon_rad']
df['dlatOT'] = df['latOTrad'] - df['lat_rad']
df['dlonOT'] = df['lonOTrad'] - df['lon_rad']

df['aSY'] =  np.sin(df['dlatSY'] / 2)**2 + np.cos(df['lat_rad']) * np.cos(df['latSYrad']) * np.sin(df['dlonSY'] / 2)**2
df['aOT'] =  np.sin(df['dlatOT'] / 2)**2 + np.cos(df['lat_rad']) * np.cos(df['latOTrad']) * np.sin(df['dlonOT'] / 2)**2
df['cSY'] = 2 * np.arctan2(np.sqrt(df['aSY']), np.sqrt(1 - df['aSY']))
df['cOT'] = 2 * np.arctan2(np.sqrt(df['aOT']), np.sqrt(1 - df['aOT']))

df['distance_SY (km)'] = R * df['cSY']
df['distance_OT (km)'] = R * df['cOT']

df = df.drop(columns = ['latSY', 'lonSY', 'latOT', 'lonOT',
       'lat_rad', 'lon_rad', 'latSYrad', 'lonSYrad', 'latOTrad', 'lonOTrad',
       'dlatSY', 'dlonSY', 'dlatOT', 'dlonOT', 'aSY', 'aOT', 'cSY', 'cOT'])
df.head()

dist = sns.displot(df.price, aspect = 2)
dist.set(xlabel = "Price (Millions of USD)", ylabel = "Number of houses")

plt.rcParams['figure.figsize'] = (10, 5)
sns.heatmap(df.corr(), cmap = 'coolwarm', annot = True)

plt.show()

max_threshold = df['price'].quantile(0.95)
max_threshold

df[df['price']>max_threshold]

min_threshold = df['price'].quantile(0.05)
min_threshold

df[df['price']<min_threshold]

df2 = df[(df.price < max_threshold) & (df.price > min_threshold)]
df2.shape

df2.columns

x = df2[['bedrooms', 'bathrooms', 'parkingSpots', 'propertySize',
         'distance_SY (km)', 'distance_OT (km)']]
y = df2[['price']]

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)


from sklearn import metrics
from sklearn import linear_model
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score

ols = linear_model.LinearRegression()
ols.fit(x_train,y_train)
np.mean(cross_val_score(ols, x_train, y_train, cv = 5, scoring = "r2"))

model = linear_model.Lasso()
print("Lasso regression score: ", np.round(np.mean(cross_val_score(model, x_train, y_train, cv = 5, scoring = "r2")), 5))

from sklearn.model_selection import GridSearchCV

alphas = [0.01, 0.1,0.5,0.75,1]
model = linear_model.Lasso()
grid_lasso = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas), cv=5)
grid_lasso.fit(x_train, y_train)
print("Lasso regression best alpha value: ", grid_lasso.best_estimator_.alpha)
print("Lasso regression with hyperparameter tuning best score: ", np.round(grid_lasso.best_score_, 5))
print("Lasso regression improvement after hyperparameter tuning: {0}%".format(np.round((1 - ((np.round(np.mean(cross_val_score(model, x_train, y_train, cv = 5, scoring = "r2")), 5)) 
                                                                                             / np.round(grid_lasso.best_score_, 5))) * 100, 5)))

model = linear_model.Ridge()
print("Ridge regression score: ", np.round(np.mean(cross_val_score(model, x_train, y_train, cv = 5, scoring = "r2")), 5))

alphas = [int(x) for x in np.linspace(1, 10, num = 20)]
model = linear_model.Ridge()
grid_ridge = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))
grid_ridge.fit(x_train, y_train)
print("Ridge regression best alpha value: ", grid_ridge.best_estimator_.alpha)
print("Ridge regression with hyperparameter tuning best score: ", np.round(grid_ridge.best_score_, 5))
print("Ridge regression improvement after hyperparameter tuning: {0}%".format(np.round((1 - ((np.round(np.mean(cross_val_score(model, x_train, y_train, cv = 5, scoring = "r2")), 5)) 
                                                                                             / np.round(grid_ridge.best_score_, 5))) * 100, 5)))

model = linear_model.ElasticNet()
print("Elastic Net regression score: ", np.round(np.mean(cross_val_score(model, x_train, y_train, cv = 5, scoring = "r2")), 5))

alphas = np.array([0.01, 0.02, 0.025, 0.05,0.1,0.5,1])
model = linear_model.ElasticNet()
grid_elastic = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))
grid_elastic.fit(x_train, y_train)

print("Elastic Net regression best alpha value: ", grid_elastic.best_estimator_.alpha)
print("Elastic Net regression with hyperparameter tuning best score: ", np.round(grid_elastic.best_score_, 5))
print("Elastic Net regression improvement after hyperparameter tuning: {0}%".format(np.round((1 - ((np.round(np.mean(cross_val_score(model, x_train, y_train, cv = 5, scoring = "r2")), 5)) 
                                                                                             / np.round(grid_elastic.best_score_, 5))) * 100, 5)))

from sklearn.ensemble import RandomForestRegressor
rf_Model = RandomForestRegressor()
print("Random Forest regression score: ", np.round(np.mean(cross_val_score(rf_Model, x_train, y_train.values.ravel(), cv = 5, scoring = "r2")), 5))

param_grid = {'n_estimators': [int(x) for x in np.linspace(25, 75, num = 3)],
               'max_features': ['auto', 'sqrt'],
               'max_depth': [2, 4, 8],
               'min_samples_split': [2, 5,10],
               'min_samples_leaf': [1, 2, 4],
               'bootstrap': [True, False]}
print(param_grid)

from sklearn.ensemble import RandomForestRegressor
rf_Model = RandomForestRegressor()
rf_Grid = GridSearchCV(estimator = rf_Model, param_grid = param_grid, cv = 5, verbose=2, n_jobs = 4)
rf_Grid.fit(x_train, y_train.values.ravel())
print(rf_Grid.best_params_)
print("Random Forest regression with hyperparameter tuning best score: ", np.round(rf_Grid.best_score_, 5))
print("Random Forest regression improvement after hyperparameter tuning: {0}%".format(np.round((1 - ((np.round(np.mean(cross_val_score(rf_Model, x_train, y_train.values.ravel(), cv = 5, scoring = "r2")), 5)) 
                                                                                             / np.round(rf_Grid.best_score_, 5))) * 100, 5)))

from sklearn.ensemble import GradientBoostingRegressor
gbr = GradientBoostingRegressor()
print("Gradient Booster regression score: ", np.round(np.mean(cross_val_score(gbr, x_train, y_train.values.ravel(), cv = 5, scoring = "r2")), 5))

param_grid = {'n_estimators': [400, 500, 600],
               'learning_rate': [0.005, 0.01, 0.02],
               'max_depth': [1, 2, 4, 8],
               'subsample': [0.6, 0.8, 1]}
print(param_grid)

model = GradientBoostingRegressor()
grid_gbr = GridSearchCV(estimator = model, param_grid = param_grid, cv = 5, verbose=2, n_jobs = 4)
grid_gbr.fit(x_train, y_train.values.ravel())
print(grid_gbr.best_params_)
print("Gradient Booster regression with hyperparameter tuning best score: ", np.round(grid_gbr.best_score_, 5))
print("Gradient Booster regression improvement after hyperparameter tuning: {0}%".format(np.round((1 - ((np.round(np.mean(cross_val_score(gbr, x_train, y_train.values.ravel(), cv = 5, scoring = "r2")), 5)) 
                                                                                             / np.round(grid_gbr.best_score_, 5))) * 100, 5)))

import xgboost
xgb = xgboost.XGBRegressor()
np.mean(cross_val_score(xgb, x_train, y_train, cv = 5, scoring = "r2"))

param_grid = {'n_estimators': [int(x) for x in np.linspace(250, 500, num = 5)],
               'learning_rate': [0.01, 0.02, 0.03],
               'max_depth': [2, 4, 8],
               'colsample_bytree': [0.5,0.75, 1],
               'subsample': [0.6,0.8, 1]}
print(param_grid)

model = xgboost.XGBRegressor()
grid_xgb = GridSearchCV(estimator = model, param_grid = param_grid, cv = 5, verbose=2, n_jobs = 4)
grid_xgb.fit(x_train, y_train)
print(grid_xgb.best_params_)
print("XGBoost regression with hyperparameter tuning best score: ", np.round(grid_xgb.best_score_, 5))
print("XGBoost Forest regression improvement after hyperparameter tuning: {0}%".format(np.round((1 - ((np.round(np.mean(cross_val_score(xgb, x_train, y_train, cv = 5, scoring = "r2")), 5)) 
                                                                                             / np.round(grid_xgb.best_score_, 5))) * 100, 5)))

from sklearn.preprocessing import PolynomialFeatures
def create_polynomial_regression_model(degree):
    poly_features = PolynomialFeatures(degree=degree)
    X_poly = poly_features.fit_transform(x_train)
    poly = LinearRegression()
    return np.mean(cross_val_score(poly, X_poly, y_train, cv=5, scoring = "r2"))
cv_scores=[]
degrees =[2,3,4, 5]
for degree in degrees:
    cv_scores.append(create_polynomial_regression_model(degree))
    
print(max(cv_scores))

fig,ax=plt.subplots(figsize=(6,6))
ax.plot(degrees,cv_scores)
ax.set_xlabel('Degree',fontsize=20)
ax.set_ylabel('R2',fontsize=20)
ax.set_title('R2 VS Degree',fontsize=25)

poly_features = PolynomialFeatures(degree=2)
X_poly = poly_features.fit_transform(x_train)
poly = LinearRegression()
poly.fit(x_train, y_train)

ols_yhat = ols.predict(x_test)
lasso_yhat  = grid_lasso.best_estimator_.predict(x_test)
ridge_yhat = grid_ridge.best_estimator_.predict(x_test)
elastic_yhat = grid_elastic.best_estimator_.predict(x_test)
forest_yhat = rf_Grid.best_estimator_.predict(x_test)
gbr_yhat = grid_gbr.predict(x_test)
xgb_yhat = grid_xgb.predict(x_test)
poly_yhat = poly.predict(x_test)

from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error

print("Ordinary Least Square accuracy: %.2f" % r2_score(y_test, ols_yhat) )
print("Lasso regression accuracy: %.2f" % r2_score(y_test, lasso_yhat) )
print("Ridge regression accuracy: %.2f" % r2_score(y_test, ridge_yhat) )
print("Elastic net regression accuracy: %.2f" % r2_score(y_test, elastic_yhat) )
print("Random forest regression accuracy: %.2f" % r2_score(y_test, forest_yhat) )
print("Gradient Booster regression accuracy: %.2f" % r2_score(y_test, gbr_yhat) )
print("XGBoost regression accuracy: %.2f" % r2_score(y_test, xgb_yhat) )
print("Polynomial regression accuracy: %.2f" % r2_score(y_test, poly_yhat) )

xgb_ytest =  y_test.reset_index(drop=True)
xgb_ypred = pd.DataFrame(xgb_yhat)
dfs = [xgb_ytest, xgb_ypred ]
xgb_df = pd.concat(dfs, axis = 1)
xgb_df.rename(columns = {0:'predictions'}, inplace = True)
xgb_df

from sklearn.ensemble import StackingRegressor
estimators = [ ('gbr', GradientBoostingRegressor(learning_rate = 0.02, max_depth = 4, 
                                                 n_estimators = 600, subsample = 0.6)),
             ('rfr', RandomForestRegressor(bootstrap = 'False', max_depth = 8, max_features = 'sqrt', min_samples_leaf = 1, min_samples_split = 2, n_estimators = 75))]

stack = StackingRegressor(estimators = estimators, final_estimator = xgboost.XGBRegressor(colsample_bytree = 0.5, learning_rate = 0.02, max_depth =8, 
                                                                                          n_estimators = 312, subsample = 0.8) )
stack.fit(x_train, y_train.values.ravel())

stack_yhat = stack.predict(x_test)

print("Stack regression accuracy: %.2f" % r2_score(y_test, stack_yhat) )

# 'bedrooms', 'bathrooms', 'parkingSpots', 'propertySize',  'distance_SY (km)', 'distance_OT (km)'

x = [[2	, 1,	1	, 250	, 5.6,	6.07 ]]
x = pd.DataFrame(x, columns = ['bedrooms', 'bathrooms', 'parkingSpots', 'propertySize',  'distance_SY (km)', 'distance_OT (km)'])

y_pred = grid_xgb.predict(x)
y_pred

import pickle

data = {"model": grid_xgb}
with open('saved_steps.pkl', 'wb') as file:
    pickle.dump(data, file)

with open('saved_steps.pkl', 'rb') as file:
    data = pickle.load(file)

regressor_loaded = data["model"]


y_pred = regressor_loaded.predict(x)
y_pred

